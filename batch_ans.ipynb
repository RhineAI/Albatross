{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=200)\n",
    "import types, torch, copy, time, random, json, math, gc\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "import gc\n",
    "\n",
    "SHOW_SPEED_PERCENTILE = 50\n",
    "\n",
    "args = types.SimpleNamespace()\n",
    "args.vocab_size = 65536\n",
    "args.head_size = 64\n",
    "\n",
    "args.MODEL_NAME = \"/models/rwkv7-g0b-13.3b-20251130-ctx8192\"\n",
    "\n",
    "print(f'\\nUsing CUDA fp16. Loading {args.MODEL_NAME} ...\\n')\n",
    "\n",
    "from reference.rwkv7 import RWKV_x070\n",
    "model = RWKV_x070(args)\n",
    "\n",
    "from reference.utils import TRIE_TOKENIZER\n",
    "tokenizer = TRIE_TOKENIZER(\"reference/rwkv_vocab_v20230424.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b5aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_questions(l: list[str]):\n",
    "    b = len(l)\n",
    "    m = []\n",
    "    lens = []\n",
    "    for s in l:\n",
    "        u = torch.tensor(tokenizer.encode(s))\n",
    "        m.append(u)\n",
    "        lens.append(len(u))\n",
    "    lmax = max(lens)\n",
    "    x = torch.zeros((len(l),lmax), dtype=torch.int32)\n",
    "    for i in range(b):\n",
    "        x[i][-lens[i]:] = m[i]\n",
    "    return (x.to(0), torch.tensor(lens, device=0, dtype=torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ef8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "sample = load(\n",
    "    name=\"sample\",\n",
    "    sources = [\"../sampling/sampling.cpp\",\"../sampling/sampling.cu\"],\n",
    "    extra_cuda_cflags=[\"-O3\", \"-res-usage\", \"--extra-device-vectorization\", \"-Xptxas -O3\"],\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0cd96e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_states = sample.setup_rand(42, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "778ed91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def samples(forward, state, logits, maxlen=8192, temp=1.0, topp=0.6, topk=-1, penalties=[0.5, 0.5, 0.996]):\n",
    "    global rand_states\n",
    "    bsz = logits.size(0) if logits.dim() == 2 else 1\n",
    "    logits = logits.to(torch.float32)\n",
    "    m = torch.empty((bsz, maxlen), device=0, dtype=torch.int32)\n",
    "    if penalties:\n",
    "        penalty_state = torch.zeros((bsz, args.vocab_size), device=0)\n",
    "        penalty_state[:, 65530:] = float('inf')\n",
    "    \n",
    "    for i in tqdm.tqdm(range(maxlen)):\n",
    "        # penalty_state_1 = copy.deepcopy(penalty_state)\n",
    "        # rand_state_1 = copy.deepcopy(rand_states)\n",
    "        if penalties == []:\n",
    "            s = sample.batch_sampling_temperature_topk_topp(logits, rand_states, temp, topk, topp)\n",
    "        else:\n",
    "            presence, repetition, decay = penalties\n",
    "            s = sample.batch_sampling_repetition_temperature_topk_topp(\n",
    "                logits, penalty_state, rand_states, presence, repetition, decay, temp, topk, topp)\n",
    "        # if (s >= 65530).any():\n",
    "        #     torch.save((\n",
    "        #         logits, penalty_state_1, rand_state_1, presence, repetition, decay, temp, topk, topp), \"./bug.pt\")\n",
    "        #     raise RuntimeError\n",
    "        m[:, i] = s\n",
    "        if i == maxlen-1:\n",
    "            return m\n",
    "        logits = (forward(s.reshape(bsz, 1), state)).to(torch.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76751c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def post(ans: bytes):\n",
    "    s = ans.decode('utf-8', errors='ignore')\n",
    "    idx1 = s.find('\\n\\n')\n",
    "    idx2 = s.find(\"<|endoftext|>\")\n",
    "    u = -1\n",
    "    if idx1 >= 0: u = min(u, idx1) if u >= 0 else idx1\n",
    "    if idx2 >= 0: u = min(u, idx2) if u >= 0 else idx2\n",
    "    if u >= 0: s = s[:u]\n",
    "    idx1 = s.find('</think>')\n",
    "    if (idx1 >= 0): s = s[idx1+8:]\n",
    "    return s.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1711468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_questions(q: list, bsz:int=20, maxlen: int=2048):\n",
    "    l = len(q)\n",
    "    a = [(len(q[i]), q[i], i) for i in range(l)]\n",
    "    a.sort(reverse=True)\n",
    "    d = [None] * l\n",
    "    for i in range(0, l, bsz):\n",
    "        s = a[i:min(i+bsz, l)]\n",
    "        ls, qs, ids = zip(*s)\n",
    "        ins, lens = tokenize_questions(qs)\n",
    "        state = model.generate_zero_state(len(qs))\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "        logits = model.forward_seq_batch_right(ins, state, lens)\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "        ans = samples(model.forward_seq_batch_1, state, logits, maxlen=maxlen)\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "        for u in range(len(qs)):\n",
    "            ss = post(tokenizer.decodeBytes(ans[u].tolist()))\n",
    "            d[ids[u]] = ss\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a086c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_format(s):\n",
    "    return f\"User: {s}\\n\\nAssistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d26e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../AlignBench/data/data_newest_release.jsonl\", \"r\", encoding=\"utf8\") as f:\n",
    "    q = [chat_format(json.loads(line)[\"question\"]) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6decf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 6143/6144 [02:41<00:00, 38.09it/s]\n",
      "100%|█████████▉| 6143/6144 [02:38<00:00, 38.77it/s]\n"
     ]
    }
   ],
   "source": [
    "ans = run_questions(q, maxlen=6144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "197122b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(ans)\n",
    "df.to_csv(\"output_13b.csv\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
